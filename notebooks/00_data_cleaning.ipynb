{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/JLX626/py-aiml-aal-portfolio-edition/main/images/Australian-Apparel-Limited-Logo.png\" alt=\"Australian Apparel Ltd\" width=\"100\"/>\n",
    "    <h2>Australian Apparel Ltd.<br/>Data Cleaning and Preprocessing</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "In this notebook, I demonstrate the data cleaning and preprocessing steps for Australian Apparel Limited's Q4 2020 sales data. My goal was to prepare the raw data for in-depth analysis, ensuring its quality and consistency. This process showcases my skills in Python, pandas, and numpy for real-world data manipulation and analysis.\n",
    "\n",
    "## Skills and Tools Demonstrated\n",
    "\n",
    "- Python programming (pandas, numpy)\n",
    "- Data cleaning and preprocessing\n",
    "- Data quality assessment and validation\n",
    "- Time series data handling\n",
    "- Data normalization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "### 1. Initial Data Inspection\n",
    "\n",
    "I began by loading the dataset and performing a comprehensive inspection to understand its structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows:\n",
      "         Date        Time State     Group  Unit  Sales\n",
      "0  1-Oct-2020     Morning    WA      Kids     8  20000\n",
      "1  1-Oct-2020     Morning    WA       Men     8  20000\n",
      "2  1-Oct-2020     Morning    WA     Women     4  10000\n",
      "3  1-Oct-2020     Morning    WA   Seniors    15  37500\n",
      "4  1-Oct-2020   Afternoon    WA      Kids     3   7500\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7560 entries, 0 to 7559\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    7560 non-null   object\n",
      " 1   Time    7560 non-null   object\n",
      " 2   State   7560 non-null   object\n",
      " 3   Group   7560 non-null   object\n",
      " 4   Unit    7560 non-null   int64 \n",
      " 5   Sales   7560 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 354.5+ KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "              Unit          Sales\n",
      "count  7560.000000    7560.000000\n",
      "mean     18.005423   45013.558201\n",
      "std      12.901403   32253.506944\n",
      "min       2.000000    5000.000000\n",
      "25%       8.000000   20000.000000\n",
      "50%      14.000000   35000.000000\n",
      "75%      26.000000   65000.000000\n",
      "max      65.000000  162500.000000\n",
      "\n",
      "Missing Values:\n",
      "Date     0\n",
      "Time     0\n",
      "State    0\n",
      "Group    0\n",
      "Unit     0\n",
      "Sales    0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "Column Names:\n",
      "Index(['Date', 'Time', 'State', 'Group', 'Unit', 'Sales'], dtype='object')\n",
      "\n",
      "Unique values in categorical columns:\n",
      "\n",
      "Date:\n",
      "['1-Oct-2020' '2-Oct-2020' '3-Oct-2020' '4-Oct-2020' '5-Oct-2020'\n",
      " '6-Oct-2020' '7-Oct-2020' '8-Oct-2020' '9-Oct-2020' '10-Oct-2020'\n",
      " '11-Oct-2020' '12-Oct-2020' '13-Oct-2020' '14-Oct-2020' '15-Oct-2020'\n",
      " '16-Oct-2020' '17-Oct-2020' '18-Oct-2020' '19-Oct-2020' '20-Oct-2020'\n",
      " '21-Oct-2020' '22-Oct-2020' '23-Oct-2020' '24-Oct-2020' '25-Oct-2020'\n",
      " '26-Oct-2020' '27-Oct-2020' '28-Oct-2020' '29-Oct-2020' '30-Oct-2020'\n",
      " '1-Nov-2020' '2-Nov-2020' '3-Nov-2020' '4-Nov-2020' '5-Nov-2020'\n",
      " '6-Nov-2020' '7-Nov-2020' '8-Nov-2020' '9-Nov-2020' '10-Nov-2020'\n",
      " '11-Nov-2020' '12-Nov-2020' '13-Nov-2020' '14-Nov-2020' '15-Nov-2020'\n",
      " '16-Nov-2020' '17-Nov-2020' '18-Nov-2020' '19-Nov-2020' '20-Nov-2020'\n",
      " '21-Nov-2020' '22-Nov-2020' '23-Nov-2020' '24-Nov-2020' '25-Nov-2020'\n",
      " '26-Nov-2020' '27-Nov-2020' '28-Nov-2020' '29-Nov-2020' '30-Nov-2020'\n",
      " '1-Dec-2020' '2-Dec-2020' '3-Dec-2020' '4-Dec-2020' '5-Dec-2020'\n",
      " '6-Dec-2020' '7-Dec-2020' '8-Dec-2020' '9-Dec-2020' '10-Dec-2020'\n",
      " '11-Dec-2020' '12-Dec-2020' '13-Dec-2020' '14-Dec-2020' '15-Dec-2020'\n",
      " '16-Dec-2020' '17-Dec-2020' '18-Dec-2020' '19-Dec-2020' '20-Dec-2020'\n",
      " '21-Dec-2020' '22-Dec-2020' '23-Dec-2020' '24-Dec-2020' '25-Dec-2020'\n",
      " '26-Dec-2020' '27-Dec-2020' '28-Dec-2020' '29-Dec-2020' '30-Dec-2020']\n",
      "\n",
      "Time:\n",
      "[' Morning' ' Afternoon' ' Evening']\n",
      "\n",
      "State:\n",
      "[' WA' ' NT' ' SA' ' VIC' ' QLD' ' NSW' ' TAS']\n",
      "\n",
      "Group:\n",
      "[' Kids' ' Men' ' Women' ' Seniors']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the data\n",
    "def inspect_data(df):\n",
    "    print(\"First few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    print(\"\\nColumn Names:\")\n",
    "    print(df.columns)\n",
    "    \n",
    "    print(\"\\nUnique values in categorical columns:\")\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].unique())\n",
    "\n",
    "# Load the data\n",
    "df = load_data('../data/raw/AusApparalSales4thQrt2020.csv')\n",
    "\n",
    "# Inspect the data\n",
    "inspect_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key findings from my initial inspection:\n",
    "- The dataset contains 7,560 entries with 6 columns\n",
    "- There are no missing values or duplicates, which is a good start\n",
    "- Categorical columns: Date, Time, State, Group\n",
    "- Numerical columns: Unit, Sales\n",
    "\n",
    "I noticed that the dataset covers sales from October 1, 2020, to December 30, 2020. Sales range from $5,000 to $162,500, with a mean of $45,013.56, while units sold range from 2 to 65, with a mean of 18 units. Interestingly, all major Australian states and territories are present, except for the Australian Capital Territory (ACT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and Transformation\n",
    "\n",
    "#### Date Column Conversion\n",
    "\n",
    "Next, I converted the 'Date' column to a datetime format. This step is crucial for any time-based analysis I might want to do later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'Date' column: datetime64[ns]\n",
      "\n",
      "First few entries of 'Date' column:\n",
      "0   2020-10-01\n",
      "1   2020-10-01\n",
      "2   2020-10-01\n",
      "3   2020-10-01\n",
      "4   2020-10-01\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Date range:\n",
      "Earliest date: 2020-10-01 00:00:00\n",
      "Latest date: 2020-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"Data type of 'Date' column:\", df['Date'].dtype)\n",
    "print(\"\\nFirst few entries of 'Date' column:\")\n",
    "print(df['Date'].head())\n",
    "\n",
    "# Additional verification\n",
    "print(\"\\nDate range:\")\n",
    "print(\"Earliest date:\", df['Date'].min())\n",
    "print(\"Latest date:\", df['Date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By converting the dates from strings (like \"1-Oct-2020\") to datetime objects, I've made it much easier to perform operations like grouping sales by week or month, or calculating the time between purchases.\n",
    "\n",
    "#### State Data Verification\n",
    "\n",
    "I then took a closer look at the 'State' column to verify our earlier observation about ACT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique states in the dataset: [' WA' ' NT' ' SA' ' VIC' ' QLD' ' NSW' ' TAS']\n",
      "ACT (Australian Capital Territory) is not present in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'State' column\n",
    "unique_states = df['State'].unique()\n",
    "print(\"Unique states in the dataset:\", unique_states)\n",
    "\n",
    "# Verify absence of ACT\n",
    "if 'ACT' not in unique_states:\n",
    "    print(\"ACT (Australian Capital Territory) is not present in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I confirmed that ACT is indeed absent from the dataset. This could be because:\n",
    "1. AAL doesn't operate in ACT\n",
    "2. There was an oversight in data collection for ACT\n",
    "3. ACT data might be combined with another state's data (often New South Wales)\n",
    "\n",
    "This is an important point to keep in mind for any state-based analysis I might do later.\n",
    "\n",
    "#### Month Column Creation\n",
    "\n",
    "To facilitate monthly trend analysis, I created a new 'Month' column derived from the 'Date' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows with new 'Month' column:\n",
      "        Date    Month\n",
      "0 2020-10-01  2020-10\n",
      "1 2020-10-01  2020-10\n",
      "2 2020-10-01  2020-10\n",
      "3 2020-10-01  2020-10\n",
      "4 2020-10-01  2020-10\n",
      "\n",
      "Unique months in the dataset:\n",
      "<PeriodArray>\n",
      "['2020-10', '2020-11', '2020-12']\n",
      "Length: 3, dtype: period[M]\n"
     ]
    }
   ],
   "source": [
    "# Create 'Month' column\n",
    "df['Month'] = df['Date'].dt.to_period('M')\n",
    "\n",
    "# Verify the new column\n",
    "print(\"First few rows with new 'Month' column:\")\n",
    "print(df[['Date', 'Month']].head())\n",
    "\n",
    "# Check unique months\n",
    "unique_months = df['Month'].unique()\n",
    "print(\"\\nUnique months in the dataset:\")\n",
    "print(unique_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new column will allow me to easily aggregate data by month without losing the granularity of the original date data.\n",
    "\n",
    "#### Consistent Formatting\n",
    "\n",
    "I then ensured consistent formatting across all columns, particularly the categorical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Time' has leading/trailing whitespaces.\n",
      "Column 'State' has leading/trailing whitespaces.\n",
      "Column 'Group' has leading/trailing whitespaces.\n",
      "\n",
      "Unique values in State:\n",
      "[' WA' ' NT' ' SA' ' VIC' ' QLD' ' NSW' ' TAS']\n",
      "\n",
      "Unique values in Group:\n",
      "[' Kids' ' Men' ' Women' ' Seniors']\n",
      "\n",
      "Unique values in Time:\n",
      "[' Morning' ' Afternoon' ' Evening']\n"
     ]
    }
   ],
   "source": [
    "# Check for leading/trailing whitespaces in string columns\n",
    "string_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in string_columns:\n",
    "    if df[col].str.strip().ne(df[col]).any():\n",
    "        print(f\"Column '{col}' has leading/trailing whitespaces.\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' has no leading/trailing whitespaces.\")\n",
    "\n",
    "# Check for inconsistent formatting in categorical variables\n",
    "categorical_columns = ['State', 'Group', 'Time']\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed any leading or trailing whitespaces from the 'State', 'Group', and 'Time' columns. This step is crucial for accurate grouping and analysis later on. Inconsistent formatting (like extra spaces) can lead to the same category being treated as different categories.\n",
    "\n",
    "#### Data Range Validation\n",
    "\n",
    "Next, I validated the ranges for the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in State after cleaning:\n",
      "['WA' 'NT' 'SA' 'VIC' 'QLD' 'NSW' 'TAS']\n",
      "\n",
      "Unique values in Group after cleaning:\n",
      "['Kids' 'Men' 'Women' 'Seniors']\n",
      "\n",
      "Unique values in Time after cleaning:\n",
      "['Morning' 'Afternoon' 'Evening']\n",
      "Column 'State' is clean of whitespace issues.\n",
      "Column 'Group' is clean of whitespace issues.\n",
      "Column 'Time' is clean of whitespace issues.\n"
     ]
    }
   ],
   "source": [
    "# Remove leading/trailing whitespaces from categorical columns\n",
    "categorical_columns = ['State', 'Group', 'Time']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# Re-check unique values after cleaning\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nUnique values in {col} after cleaning:\")\n",
    "    print(df[col].unique())\n",
    "\n",
    "# Verify no more whitespace issues\n",
    "for col in categorical_columns:\n",
    "    if df[col].str.strip().ne(df[col]).any():\n",
    "        print(f\"Column '{col}' still has whitespace issues.\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' is clean of whitespace issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I confirmed that all 'Unit' and 'Sales' values are within expected ranges. This step helps identify any potential outliers or data entry errors that might skew our analysis.\n",
    "\n",
    "### 3. Data Normalization and Final Preprocessing\n",
    "\n",
    "As a final step, I applied normalization to the 'Sales' and 'Unit' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for Unit:\n",
      "count    7560.000000\n",
      "mean       18.005423\n",
      "std        12.901403\n",
      "min         2.000000\n",
      "25%         8.000000\n",
      "50%        14.000000\n",
      "75%        26.000000\n",
      "max        65.000000\n",
      "Name: Unit, dtype: float64\n",
      "No negative values found in Unit\n",
      "\n",
      "Statistics for Sales:\n",
      "count      7560.000000\n",
      "mean      45013.558201\n",
      "std       32253.506944\n",
      "min        5000.000000\n",
      "25%       20000.000000\n",
      "50%       35000.000000\n",
      "75%       65000.000000\n",
      "max      162500.000000\n",
      "Name: Sales, dtype: float64\n",
      "No negative values found in Sales\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Validate data ranges for numerical columns\n",
    "for col in numerical_columns:\n",
    "    print(f\"\\nStatistics for {col}:\")\n",
    "    print(df[col].describe())\n",
    "    \n",
    "    # Check for negative values in columns where it doesn't make sense\n",
    "    if col in ['Unit', 'Sales']:\n",
    "        neg_count = (df[col] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            print(f\"Warning: {neg_count} negative values found in {col}\")\n",
    "        else:\n",
    "            print(f\"No negative values found in {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Sales         Unit\n",
      "count    7560.000000  7560.000000\n",
      "mean    45013.558201    18.005423\n",
      "std     32253.506944    12.901403\n",
      "min      5000.000000     2.000000\n",
      "25%     20000.000000     8.000000\n",
      "50%     35000.000000    14.000000\n",
      "75%     65000.000000    26.000000\n",
      "max    162500.000000    65.000000\n",
      "\n",
      "First few rows with normalized columns:\n",
      "   Sales  Sales_Normalized  Unit  Unit_Normalized\n",
      "0  20000          0.095238     8         0.095238\n",
      "1  20000          0.095238     8         0.095238\n",
      "2  10000          0.031746     4         0.031746\n",
      "3  37500          0.206349    15         0.206349\n",
      "4   7500          0.015873     3         0.015873\n",
      "\n",
      "Summary of normalized columns:\n",
      "       Sales_Normalized  Unit_Normalized\n",
      "count       7560.000000      7560.000000\n",
      "mean           0.254054         0.254054\n",
      "std            0.204784         0.204784\n",
      "min            0.000000         0.000000\n",
      "25%            0.095238         0.095238\n",
      "50%            0.190476         0.190476\n",
      "75%            0.380952         0.380952\n",
      "max            1.000000         1.000000\n"
     ]
    }
   ],
   "source": [
    "# Check for potential outliers or incorrect data\n",
    "print(df[['Sales', 'Unit']].describe())\n",
    "\n",
    "# Simple normalization for 'Sales' and 'Unit' columns\n",
    "df['Sales_Normalized'] = (df['Sales'] - df['Sales'].min()) / (df['Sales'].max() - df['Sales'].min())\n",
    "df['Unit_Normalized'] = (df['Unit'] - df['Unit'].min()) / (df['Unit'].max() - df['Unit'].min())\n",
    "\n",
    "print(\"\\nFirst few rows with normalized columns:\")\n",
    "print(df[['Sales', 'Sales_Normalized', 'Unit', 'Unit_Normalized']].head())\n",
    "\n",
    "print(\"\\nSummary of normalized columns:\")\n",
    "print(df[['Sales_Normalized', 'Unit_Normalized']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used min-max normalization to bring these columns to a common scale of 0 to 1. This step is particularly useful when we want to compare or combine features that are on different scales.\n",
    "\n",
    "Key observations from the normalized data:\n",
    "- The mean of normalized data is about 0.254, or 25.4% of the maximum values\n",
    "- The median is 0.190, indicating a right-skewed distribution\n",
    "- The standard deviation is 0.205, suggesting a moderate spread of values\n",
    "\n",
    "Finally, I saved the cleaned and preprocessed dataset for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "cleaned_data = df\n",
    "cleaned_data.to_csv('../data/processed/cleaned_sales_data.csv', index=False)\n",
    "print(\"Cleaned data saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Through this process, I:\n",
    "1. Demonstrated my ability to handle and clean real-world sales data\n",
    "2. Implemented robust data preprocessing techniques, including date conversion, categorical data cleaning, and normalization\n",
    "3. Prepared a high-quality dataset ready for advanced analytics and visualization\n",
    "4. Identified potential areas for further investigation, such as the right-skewed sales distribution and the absence of ACT data\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "With this cleaned and normalized dataset, I am now ready for more in-depth analysis, including:\n",
    "- Analyzing sales trends by state and customer group\n",
    "- Performing time series analysis of monthly sales patterns\n",
    "- Investigating the right-skewed sales distribution we observed\n",
    "\n",
    "To give a glimpse of potential analysis directions, I've included some preliminary data groupings below. These initial groupings set the stage for more comprehensive explorations in subsequent notebooks. The work done here in cleaning and preprocessing the data will be crucial in ensuring the accuracy and reliability of any future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics by State:\n",
      "        count          mean           std      min      25%      50%  \\\n",
      "State                                                                  \n",
      "NSW    1080.0  69416.666667  20626.651646  30000.0  52500.0  70000.0   \n",
      "NT     1080.0  20907.407407   8961.907893   5000.0  15000.0  20000.0   \n",
      "QLD    1080.0  30942.129630  13344.638002   7500.0  20000.0  30000.0   \n",
      "SA     1080.0  54497.685185  17460.965183  25000.0  40000.0  52500.0   \n",
      "TAS    1080.0  21074.074074   9024.684205   5000.0  15000.0  20000.0   \n",
      "VIC    1080.0  97745.370370  26621.597092  50000.0  77500.0  95000.0   \n",
      "WA     1080.0  20511.574074   9231.905897   5000.0  12500.0  20000.0   \n",
      "\n",
      "            75%       max  \n",
      "State                      \n",
      "NSW     85000.0  112500.0  \n",
      "NT      27500.0   37500.0  \n",
      "QLD     40000.0   62500.0  \n",
      "SA      67500.0   87500.0  \n",
      "TAS     27500.0   37500.0  \n",
      "VIC    112500.0  162500.0  \n",
      "WA      27500.0   37500.0  \n",
      "\n",
      "Summary statistics by Customer Group:\n",
      "          count          mean           std     min      25%      50%  \\\n",
      "Group                                                                   \n",
      "Kids     1890.0  45011.904762  31871.491085  5000.0  20000.0  35000.0   \n",
      "Men      1890.0  45370.370370  32177.180712  5000.0  20000.0  35000.0   \n",
      "Seniors  1890.0  44464.285714  32195.360017  5000.0  20000.0  35000.0   \n",
      "Women    1890.0  45207.671958  32781.639869  5000.0  20000.0  35000.0   \n",
      "\n",
      "             75%       max  \n",
      "Group                       \n",
      "Kids     65000.0  162500.0  \n",
      "Men      65000.0  160000.0  \n",
      "Seniors  62500.0  162500.0  \n",
      "Women    67500.0  162500.0  \n"
     ]
    }
   ],
   "source": [
    "# Group data by State\n",
    "state_groups = cleaned_data.groupby('State')\n",
    "\n",
    "# Display summary statistics for each state\n",
    "print(\"Summary statistics by State:\")\n",
    "print(state_groups['Sales'].describe())\n",
    "\n",
    "# Group data by Customer Group\n",
    "customer_groups = cleaned_data.groupby('Group')\n",
    "\n",
    "# Display summary statistics for each customer group\n",
    "print(\"\\nSummary statistics by Customer Group:\")\n",
    "print(customer_groups['Sales'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Sales Summaries:\n",
      "     Month  Total_Sales  Avg_Daily_Sales  Num_Transactions  Total_Units  \\\n",
      "0  2020-10    114290000     45353.174603              2520        45716   \n",
      "1  2020-11     90682500     35985.119048              2520        36273   \n",
      "2  2020-12    135330000     53702.380952              2520        54132   \n",
      "\n",
      "   Avg_Daily_Units  \n",
      "0        18.141270  \n",
      "1        14.394048  \n",
      "2        21.480952  \n",
      "\n",
      "Monthly Sales Summaries with Growth Rate:\n",
      "     Month  Total_Sales  Avg_Daily_Sales  Num_Transactions  Total_Units  \\\n",
      "0  2020-10    114290000     45353.174603              2520        45716   \n",
      "1  2020-11     90682500     35985.119048              2520        36273   \n",
      "2  2020-12    135330000     53702.380952              2520        54132   \n",
      "\n",
      "   Avg_Daily_Units  Sales_Growth_Rate  \n",
      "0        18.141270                NaN  \n",
      "1        14.394048         -20.655788  \n",
      "2        21.480952          49.234968  \n"
     ]
    }
   ],
   "source": [
    "# Ensure 'Date' is in datetime format\n",
    "cleaned_data['Date'] = pd.to_datetime(cleaned_data['Date'])\n",
    "\n",
    "# Create a 'Month' column\n",
    "cleaned_data['Month'] = cleaned_data['Date'].dt.to_period('M')\n",
    "\n",
    "# Group by Month and calculate monthly summaries\n",
    "monthly_sales = cleaned_data.groupby('Month').agg({\n",
    "    'Sales': ['sum', 'mean', 'count'],\n",
    "    'Unit': ['sum', 'mean']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "monthly_sales.columns = ['Total_Sales', 'Avg_Daily_Sales', 'Num_Transactions', 'Total_Units', 'Avg_Daily_Units']\n",
    "\n",
    "# Reset index to make 'Month' a column\n",
    "monthly_sales = monthly_sales.reset_index()\n",
    "\n",
    "# Display the monthly summaries\n",
    "print(\"Monthly Sales Summaries:\")\n",
    "print(monthly_sales)\n",
    "\n",
    "# Calculate month-over-month growth rate for Total_Sales\n",
    "monthly_sales['Sales_Growth_Rate'] = monthly_sales['Total_Sales'].pct_change() * 100\n",
    "\n",
    "# Display the updated monthly summaries with growth rate\n",
    "print(\"\\nMonthly Sales Summaries with Growth Rate:\")\n",
    "print(monthly_sales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
